{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptation of Gluon's Self-attentive Sentence Embedding\n",
    "\n",
    "#### Eryk Wdowiak and Eric Adsetts\n",
    "\n",
    "we are adapting [Gluon tutorial](https://gluon-nlp.mxnet.io/examples/sentence_embedding/self_attentive_sentence_embedding.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import mxnet as mx\n",
    "import multiprocessing as mp\n",
    "import gluonnlp as nlp\n",
    "\n",
    "from mxnet import gluon, nd, init\n",
    "from mxnet.gluon import nn, rnn\n",
    "from mxnet import autograd, gluon, nd\n",
    "\n",
    "# fixed random number seed\n",
    "np.random.seed(2018)\n",
    "mx.random.seed(2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load subword split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  load CSV files\n",
    "data_train = pd.read_csv('dataset/judge-1377884607_zz_data-train.csv')\n",
    "data_test = pd.read_csv('dataset/judge-1377884607_zz_data-test.csv')\n",
    "\n",
    "##  load subword split tweets\n",
    "tweets_train = pd.read_csv('dataset/judge-1377884607_zz_tweets-train_v2-bpe.txt',header=None)\n",
    "tweets_test = pd.read_csv('dataset/judge-1377884607_zz_tweets-test_v2-bpe.txt',header=None)\n",
    "\n",
    "##  replace tweet column\n",
    "data_train['tweet'] = tweets_train\n",
    "data_test['tweet'] = tweets_test\n",
    "del tweets_train, tweets_test\n",
    "\n",
    "##  make sentiment a zero-based integer\n",
    "emo_int_dict = {'negative':0, 'neutral':1, 'positive':2 } \n",
    "data_train['emot_int'] = data_train['emotion'].replace(emo_int_dict)\n",
    "data_test['emot_int'] = data_test['emotion'].replace(emo_int_dict)\n",
    "del emo_int_dict\n",
    "\n",
    "##  and drop the direction column\n",
    "data_train = data_train.drop(labels='direction',axis='columns')\n",
    "data_test = data_test.drop(labels='direction',axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  bind tweets and labels together\n",
    "train_dataset = [[text, int(label)] for text, label in zip(data_train['tweet'], data_train['emot_int'])]\n",
    "valid_dataset = [[text, int(label)] for text, label in zip(data_test['tweet'], data_test['emot_int'])]\n",
    "# len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  ClipSequence takes list as input and returns list with max. length 100\n",
    "length_clip = nlp.data.ClipSequence(100)\n",
    "\n",
    "def preprocess(dataset):\n",
    "    ##  get data and label\n",
    "    data, label = dataset[0], dataset[1]\n",
    "    \n",
    "    ##  clip the length of review words\n",
    "    data = length_clip(word_tokenize(data))\n",
    "    return data, label\n",
    "\n",
    "def get_length(dataset):\n",
    "    return float(len(dataset[0]))\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    with mp.Pool() as pool:\n",
    "        # Each sample is processed in an asynchronous manner.\n",
    "        dataset = gluon.data.SimpleDataset(pool.map(preprocess, dataset))\n",
    "        lengths = gluon.data.SimpleDataset(pool.map(get_length, dataset))\n",
    "    return dataset, lengths\n",
    "\n",
    "## preprocess training set and validation set\n",
    "train_dataset, train_data_lengths = preprocess_dataset(train_dataset)\n",
    "valid_dataset, valid_data_lengths = preprocess_dataset(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  create vocabulary\n",
    "train_seqs = [sample[0] for sample in train_dataset]\n",
    "counter = nlp.data.count_tokens(list(itertools.chain.from_iterable(train_seqs)))\n",
    "\n",
    "##  limit vocabulary, should already be limited to 3,000 words, but give margin\n",
    "vocab = nlp.Vocab(counter, max_size=3030)\n",
    "\n",
    "##  load pre-trained GloVe (word) embedding with 300 dimensions\n",
    "embedding_weights = nlp.embedding.GloVe(embedding_root='dataset',source='glove.6B.300d')\n",
    "vocab.set_embedding(embedding_weights)\n",
    "# print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  token to index\n",
    "def token_to_idx(dataset):\n",
    "    return vocab[dataset[0]], dataset[1]\n",
    "\n",
    "##  return token index (or list of token indices) according to the vocabulary\n",
    "with mp.Pool() as pool:\n",
    "    train_dataset = pool.map(token_to_idx, train_dataset)\n",
    "    valid_dataset = pool.map(token_to_idx, valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucketing, mini-batches, and the `DataLoader`\n",
    "\n",
    "Since each sentence may have a different length, we need to use `Pad` to fill the sentences in a mini-batch to equal lengths so that the data can be quickly tensored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FixedBucketSampler:\n",
      "  sample_num=7273, batch_num=81\n",
      "  key=[19, 28, 37, 46, 55, 64, 73, 82, 91, 100]\n",
      "  cnt=[1770, 1867, 1593, 1032, 555, 275, 116, 39, 18, 8]\n",
      "  batch_size=[168, 114, 86, 69, 64, 64, 64, 64, 64, 64]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "bucket_num = 10\n",
    "bucket_ratio = 0.5\n",
    "\n",
    "\n",
    "def get_dataloader():\n",
    "\n",
    "    # Construct the DataLoader Pad data, stack label and lengths\n",
    "    batchify_fn = nlp.data.batchify.Tuple(\n",
    "        nlp.data.batchify.Pad(axis=0, pad_val=0),\n",
    "        nlp.data.batchify.Stack())\n",
    "\n",
    "    # In this example, we use a FixedBucketSampler,\n",
    "    # which assigns each data sample to a fixed bucket based on its length.\n",
    "    batch_sampler = nlp.data.sampler.FixedBucketSampler(\n",
    "        train_data_lengths,\n",
    "        batch_size=batch_size,\n",
    "        num_buckets=bucket_num,\n",
    "        ratio=bucket_ratio,\n",
    "        shuffle=True)\n",
    "    print(batch_sampler.stats())\n",
    "\n",
    "    # Training set DataLoader\n",
    "    train_dataloader = gluon.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        batchify_fn=batchify_fn)\n",
    "    # Validation set DataLoader\n",
    "    valid_dataloader = gluon.data.DataLoader(\n",
    "        dataset=valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        batchify_fn=batchify_fn)\n",
    "    return train_dataloader, valid_dataloader\n",
    "\n",
    "train_dataloader, valid_dataloader = get_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the model and outlining the model's structure\n",
    "\n",
    "construct the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A custom attention layer\n",
    "class SelfAttention(nn.HybridBlock):\n",
    "    def __init__(self, att_unit, att_hops, **kwargs):\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.ut_dense = nn.Dense(att_unit, activation='tanh', flatten=False)\n",
    "            self.et_dense = nn.Dense(att_hops, activation=None, flatten=False)\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        # x shape: [batch_size, seq_len, embedding_width]\n",
    "        # ut shape: [batch_size, seq_len, att_unit]\n",
    "        ut = self.ut_dense(x)\n",
    "        # et shape: [batch_size, seq_len, att_hops]\n",
    "        et = self.et_dense(ut)\n",
    "\n",
    "        # att shape: [batch_size,  att_hops, seq_len]\n",
    "        att = F.softmax(F.transpose(et, axes=(0, 2, 1)), axis=-1)\n",
    "        # output shape [batch_size, att_hops, embedding_width]\n",
    "        output = F.batch_dot(att, x)\n",
    "\n",
    "        return output, att"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the number of samples for labels are very unbalanced, applying different weights on different labels may improve the performance of the model significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSoftmaxCE(nn.Block):\n",
    "    def __init__(self, sparse_label=True, from_logits=False,  **kwargs):\n",
    "        super(WeightedSoftmaxCE, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.sparse_label = sparse_label\n",
    "            self.from_logits = from_logits\n",
    "\n",
    "    def forward(self, pred, label, class_weight, depth=None):\n",
    "        if self.sparse_label:\n",
    "            label = nd.reshape(label, shape=(-1, ))\n",
    "            label = nd.one_hot(label, depth)\n",
    "        if not self.from_logits:\n",
    "            pred = nd.log_softmax(pred, -1)\n",
    "\n",
    "        weight_label = nd.broadcast_mul(label, class_weight)\n",
    "        loss = -nd.sum(pred * weight_label, axis=-1)\n",
    "\n",
    "        # return nd.mean(loss, axis=0, exclude=True)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the basic model characteristics in a self-attentive bi-LSTM model, and configure the layers and dropout, as well as how the model feeds forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentiveBiLSTM(nn.HybridBlock):\n",
    "    def __init__(self, vocab_len, embsize, nhidden, nlayers, natt_unit, natt_hops, nfc, nclass,\n",
    "                 drop_prob, pool_way, prune_p=None, prune_q=None, **kwargs):\n",
    "        super(SelfAttentiveBiLSTM, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.embedding_layer = nn.Embedding(vocab_len, embsize)\n",
    "            self.bilstm = rnn.LSTM(nhidden, num_layers=nlayers, dropout=drop_prob, bidirectional=True)\n",
    "            self.att_encoder = SelfAttention(natt_unit, natt_hops)\n",
    "            self.dense = nn.Dense(nfc, activation='tanh')\n",
    "            self.output_layer = nn.Dense(nclass)\n",
    "\n",
    "            self.dense_p, self.dense_q = None, None\n",
    "            if all([prune_p, prune_q]):\n",
    "                self.dense_p = nn.Dense(prune_p, activation='tanh', flatten=False)\n",
    "                self.dense_q = nn.Dense(prune_q, activation='tanh', flatten=False)\n",
    "\n",
    "            self.drop_prob = drop_prob\n",
    "            self.pool_way = pool_way\n",
    "\n",
    "    def hybrid_forward(self, F, inp):\n",
    "        # input_embed: [batch, len, emsize]\n",
    "        inp_embed = self.embedding_layer(inp)\n",
    "        h_output = self.bilstm(F.transpose(inp_embed, axes=(1, 0, 2)))\n",
    "        # att_output: [batch, att_hops, emsize]\n",
    "        att_output, att = self.att_encoder(F.transpose(h_output, axes=(1, 0, 2)))\n",
    "\n",
    "        dense_input = None\n",
    "        if self.pool_way == 'flatten':\n",
    "            dense_input = F.Dropout(F.flatten(att_output), self.drop_prob)\n",
    "        elif self.pool_way == 'mean':\n",
    "            dense_input = F.Dropout(F.mean(att_output, axis=1), self.drop_prob)\n",
    "        elif self.pool_way == 'prune' and all([self.dense_p, self.dense_q]):\n",
    "            # p_section: [batch, att_hops, prune_p]\n",
    "            p_section = self.dense_p(att_output)\n",
    "            # q_section: [batch, emsize, prune_q]\n",
    "            q_section = self.dense_q(F.transpose(att_output, axes=(0, 2, 1)))\n",
    "            dense_input = F.Dropout(F.concat(F.flatten(p_section), F.flatten(q_section), dim=-1), self.drop_prob)\n",
    "\n",
    "        dense_out = self.dense(dense_input)\n",
    "        output = self.output_layer(F.Dropout(dense_out, self.drop_prob))\n",
    "\n",
    "        return output, att"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the parameters and assembling the model\n",
    "\n",
    "The resulting `M` is a matrix, and the way to classify this matrix is `flatten`-ing, `mean`-ing or `prune`-ing. Pruning is an effective way of trimming parameters that was proposed in the original paper, and has been implemented for our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len = len(vocab)\n",
    "emsize = 300           # word embedding size\n",
    "nhidden = 150   # 300  # lstm hidden_dim\n",
    "nlayers = 2            # lstm layers\n",
    "natt_unit = 150 # 300  # the hidden_units of attention layer\n",
    "natt_hops = 2          # the channels of attention\n",
    "nfc = 256       # 256\n",
    "nclass = 3\n",
    "\n",
    "drop_prob = 0.5\n",
    "pool_way = 'flatten'  # The way to handle M\n",
    "prune_p = None\n",
    "prune_q = None\n",
    "\n",
    "model = SelfAttentiveBiLSTM(vocab_len, emsize, nhidden, nlayers,\n",
    "                            natt_unit, natt_hops, nfc, nclass,\n",
    "                            drop_prob, pool_way, prune_p, prune_q)\n",
    "\n",
    "model.initialize(init=init.Xavier(), ctx=mx.cpu())\n",
    "model.hybridize()\n",
    "\n",
    "# Attach a pre-trained glove word vector to the embedding layer\n",
    "model.embedding_layer.weight.set_data(vocab.embedding.idx_to_vec)\n",
    "# fixed the layer\n",
    "model.embedding_layer.collect_params().setattr('grad_req', 'null')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "standard loss function below includes penalty coefficient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(x, y, model, loss, class_weight, penal_coeff):\n",
    "    pred, att = model(x)\n",
    "    if loss_name == 'sce':\n",
    "        l = loss(pred, y)\n",
    "    elif loss_name == 'wsce':\n",
    "        l = loss(pred, y, class_weight, class_weight.shape[0])\n",
    "\n",
    "    # penalty\n",
    "    diversity_penalty = nd.batch_dot(att, nd.transpose(att, axes=(0, 2, 1))\n",
    "                        ) - nd.eye(att.shape[1], ctx=att.context)\n",
    "    l = l + penal_coeff * diversity_penalty.norm(axis=(1, 2))\n",
    "\n",
    "    return pred, l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define what one epoch of training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch(data_iter, model, loss, trainer, ctx, is_train, epoch,\n",
    "              penal_coeff=0.0, clip=None, class_weight=None, loss_name='wsce'):\n",
    "\n",
    "    loss_val = 0.\n",
    "    total_pred = []\n",
    "    total_true = []\n",
    "    n_batch = 0\n",
    "\n",
    "    for batch_x, batch_y in data_iter:\n",
    "        batch_x = batch_x.as_in_context(ctx)\n",
    "        batch_y = batch_y.as_in_context(ctx)\n",
    "\n",
    "        if is_train:\n",
    "            with autograd.record():\n",
    "                batch_pred, l = calculate_loss(batch_x, batch_y, model, loss, class_weight, penal_coeff)\n",
    "\n",
    "            # backward calculate\n",
    "            l.backward()\n",
    "\n",
    "            # clip gradient\n",
    "            clip_params = [p.data() for p in model.collect_params().values()]\n",
    "            if clip is not None:\n",
    "                norm = nd.array([0.0], ctx)\n",
    "                for param in clip_params:\n",
    "                    if param.grad is not None:\n",
    "                        norm += (param.grad ** 2).sum()\n",
    "                norm = norm.sqrt().asscalar()\n",
    "                if norm > clip:\n",
    "                    for param in clip_params:\n",
    "                        if param.grad is not None:\n",
    "                            param.grad[:] *= clip / norm\n",
    "\n",
    "            # update parmas\n",
    "            trainer.step(batch_x.shape[0])\n",
    "\n",
    "        else:\n",
    "            batch_pred, l = calculate_loss(batch_x, batch_y, model, loss, class_weight, penal_coeff)\n",
    "\n",
    "        # keep result for metric\n",
    "        batch_pred = nd.argmax(nd.softmax(batch_pred, axis=1), axis=1).asnumpy()\n",
    "        batch_true = np.reshape(batch_y.asnumpy(), (-1, ))\n",
    "        total_pred.extend(batch_pred.tolist())\n",
    "        total_true.extend(batch_true.tolist())\n",
    "\n",
    "        batch_loss = l.mean().asscalar()\n",
    "\n",
    "        n_batch += 1\n",
    "        loss_val += batch_loss\n",
    "\n",
    "        # check the result of traing phase\n",
    "        if is_train and n_batch % 10 == 0:\n",
    "            print('epoch %d, batch %d, batch_train_loss %.4f, batch_train_acc %.3f' %\n",
    "                  (epoch, n_batch, batch_loss, accuracy_score(batch_true, batch_pred)))\n",
    "\n",
    "    # metric\n",
    "    F1 = f1_score(np.array(total_true), np.array(total_pred), average='weighted')\n",
    "    acc = accuracy_score(np.array(total_true), np.array(total_pred))\n",
    "    loss_val /= n_batch\n",
    "\n",
    "    if is_train:\n",
    "        # save parameters\n",
    "        ot_file = 'params/params.' + f'{epoch:02}'\n",
    "        model.save_parameters(ot_file)\n",
    "        print('saved parameters to: ' + ot_file)\n",
    "\n",
    "        ##  print statistics\n",
    "        print('epoch %d, learning_rate %.5f \\n\\t train_loss %.4f, acc_train %.3f, F1_train %.3f, ' %\n",
    "              (epoch, trainer.learning_rate, loss_val, acc, F1))\n",
    "        \n",
    "        ## reduce the learning rate\n",
    "        if epoch % 2 == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * 0.9)\n",
    "    else:\n",
    "        ##  print statistics\n",
    "        print('\\t valid_loss %.4f, acc_valid %.3f, F1_valid %.3f, ' % (loss_val, acc, F1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we include a helper method `train_valid` which combines the one epoch for the training data as well as the validation data, using the `is_train` boolean to swap between the two modes we discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid(data_iter_train, data_iter_valid, model, loss, trainer, ctx, nepochs,\n",
    "                penal_coeff=0.0, clip=None, class_weight=None, loss_name='wsce'):\n",
    "    \n",
    "    print('-'*60)\n",
    "    for epoch in range(1, nepochs+1):\n",
    "        print('BEGIN epoch '+str(epoch))\n",
    "        start = time.time()\n",
    "        # train\n",
    "        is_train = True\n",
    "        one_epoch(data_iter_train, model, loss, trainer, ctx, is_train,\n",
    "                  epoch, penal_coeff, clip, class_weight, loss_name)\n",
    "\n",
    "        # valid\n",
    "        is_train = False\n",
    "        one_epoch(data_iter_valid, model, loss, trainer, ctx, is_train,\n",
    "                  epoch, penal_coeff, clip, class_weight, loss_name)\n",
    "        end = time.time()\n",
    "        print('time %.2f sec' % (end-start))\n",
    "        print('END epoch '+str(epoch))\n",
    "        print('-'*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Now that we are actually training the model, we use `WeightedSoftmaxCE` to alleviate the problem of data categorical imbalance. We perform statistical analysis on the data in advance to retrieve a set of `class_weight`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = None\n",
    "loss_name = 'wsce'\n",
    "optim = 'adam'\n",
    "lr = 0.001\n",
    "penal_coeff = 0.1\n",
    "clip = 0.5\n",
    "nepochs = 5\n",
    "\n",
    "trainer = gluon.Trainer(model.collect_params(), optim, {'learning_rate': lr})\n",
    "\n",
    "if loss_name == 'sce':\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "elif loss_name == 'wsce':\n",
    "    loss = WeightedSoftmaxCE()\n",
    "    ##  the value of class_weight is obtained by counting data in advance. \n",
    "    ##  It can be seen as a hyperparameter.\n",
    "    class_weight = nd.array([0.618, 6.121, 3.261], ctx=mx.cpu())\n",
    "\n",
    "##  data_train['emot_int'].value_counts() / data_train['emot_int'].value_counts().sum()\n",
    "##  1    0.612127\n",
    "##  2    0.326138\n",
    "##  0    0.061735\n",
    "##  Name: emot_int, dtype: float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  train and validate\n",
    "\n",
    "We've simplified our lives earlier by creating the necessary helper methods so our training is as simple as the below line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "BEGIN epoch 1\n",
      "epoch 1, batch 10, batch_train_loss 3.2587, batch_train_acc 0.547\n",
      "epoch 1, batch 20, batch_train_loss 3.0376, batch_train_acc 0.593\n",
      "epoch 1, batch 30, batch_train_loss 3.2029, batch_train_acc 0.523\n",
      "epoch 1, batch 40, batch_train_loss 2.9639, batch_train_acc 0.667\n",
      "epoch 1, batch 50, batch_train_loss 3.1060, batch_train_acc 0.625\n",
      "epoch 1, batch 60, batch_train_loss 3.3698, batch_train_acc 0.605\n",
      "epoch 1, batch 70, batch_train_loss 2.9044, batch_train_acc 0.614\n",
      "epoch 1, batch 80, batch_train_loss 2.5593, batch_train_acc 0.689\n",
      "saved parameters to: params/params.01\n",
      "epoch 1, learning_rate 0.00100 \n",
      "\t train_loss 3.2066, acc_train 0.596, F1_train 0.475, \n",
      "\t valid_loss 2.9671, acc_valid 0.600, F1_valid 0.450, \n",
      "time 278.66 sec\n",
      "END epoch 1\n",
      "------------------------------------------------------------\n",
      "BEGIN epoch 2\n",
      "epoch 2, batch 10, batch_train_loss 2.8386, batch_train_acc 0.702\n",
      "epoch 2, batch 20, batch_train_loss 2.9477, batch_train_acc 0.640\n",
      "epoch 2, batch 30, batch_train_loss 3.4834, batch_train_acc 0.435\n",
      "epoch 2, batch 40, batch_train_loss 3.0277, batch_train_acc 0.605\n",
      "epoch 2, batch 50, batch_train_loss 2.9713, batch_train_acc 0.605\n",
      "epoch 2, batch 60, batch_train_loss 3.2968, batch_train_acc 0.544\n",
      "epoch 2, batch 70, batch_train_loss 3.1161, batch_train_acc 0.547\n",
      "epoch 2, batch 80, batch_train_loss 2.9790, batch_train_acc 0.623\n",
      "saved parameters to: params/params.02\n",
      "epoch 2, learning_rate 0.00100 \n",
      "\t train_loss 3.0711, acc_train 0.607, F1_train 0.469, \n",
      "\t valid_loss 2.9368, acc_valid 0.600, F1_valid 0.450, \n",
      "time 269.58 sec\n",
      "END epoch 2\n",
      "------------------------------------------------------------\n",
      "BEGIN epoch 3\n",
      "epoch 3, batch 10, batch_train_loss 3.2831, batch_train_acc 0.535\n",
      "epoch 3, batch 20, batch_train_loss 2.8277, batch_train_acc 0.594\n",
      "epoch 3, batch 30, batch_train_loss 2.8606, batch_train_acc 0.641\n",
      "epoch 3, batch 40, batch_train_loss 3.0705, batch_train_acc 0.579\n",
      "epoch 3, batch 50, batch_train_loss 2.7530, batch_train_acc 0.679\n",
      "epoch 3, batch 60, batch_train_loss 2.7532, batch_train_acc 0.632\n",
      "epoch 3, batch 70, batch_train_loss 2.9292, batch_train_acc 0.640\n",
      "epoch 3, batch 80, batch_train_loss 2.7501, batch_train_acc 0.656\n",
      "saved parameters to: params/params.03\n",
      "epoch 3, learning_rate 0.00090 \n",
      "\t train_loss 3.0068, acc_train 0.611, F1_train 0.465, \n",
      "\t valid_loss 3.0039, acc_valid 0.600, F1_valid 0.450, \n",
      "time 269.24 sec\n",
      "END epoch 3\n",
      "------------------------------------------------------------\n",
      "BEGIN epoch 4\n",
      "epoch 4, batch 10, batch_train_loss 3.1471, batch_train_acc 0.523\n",
      "epoch 4, batch 20, batch_train_loss 3.3158, batch_train_acc 0.531\n",
      "epoch 4, batch 30, batch_train_loss 2.8758, batch_train_acc 0.616\n",
      "epoch 4, batch 40, batch_train_loss 2.7657, batch_train_acc 0.696\n",
      "epoch 4, batch 50, batch_train_loss 3.1282, batch_train_acc 0.570\n",
      "epoch 4, batch 60, batch_train_loss 2.8576, batch_train_acc 0.673\n",
      "epoch 4, batch 70, batch_train_loss 2.8067, batch_train_acc 0.685\n",
      "epoch 4, batch 80, batch_train_loss 2.7789, batch_train_acc 0.625\n",
      "saved parameters to: params/params.04\n",
      "epoch 4, learning_rate 0.00090 \n",
      "\t train_loss 3.0085, acc_train 0.612, F1_train 0.465, \n",
      "\t valid_loss 2.9436, acc_valid 0.600, F1_valid 0.450, \n",
      "time 272.49 sec\n",
      "END epoch 4\n",
      "------------------------------------------------------------\n",
      "BEGIN epoch 5\n",
      "epoch 5, batch 10, batch_train_loss 3.1118, batch_train_acc 0.500\n",
      "epoch 5, batch 20, batch_train_loss 2.6956, batch_train_acc 0.711\n",
      "epoch 5, batch 30, batch_train_loss 2.6456, batch_train_acc 0.726\n",
      "epoch 5, batch 40, batch_train_loss 2.8647, batch_train_acc 0.609\n",
      "epoch 5, batch 50, batch_train_loss 2.7724, batch_train_acc 0.684\n",
      "epoch 5, batch 60, batch_train_loss 3.0504, batch_train_acc 0.594\n",
      "epoch 5, batch 70, batch_train_loss 2.7716, batch_train_acc 0.679\n",
      "epoch 5, batch 80, batch_train_loss 2.1349, batch_train_acc 0.875\n",
      "saved parameters to: params/params.05\n",
      "epoch 5, learning_rate 0.00081 \n",
      "\t train_loss 2.9526, acc_train 0.612, F1_train 0.465, \n",
      "\t valid_loss 3.0472, acc_valid 0.600, F1_valid 0.450, \n",
      "time 272.05 sec\n",
      "END epoch 5\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# train and validate\n",
    "train_valid(train_dataloader, valid_dataloader, model, loss, trainer, mx.cpu(), nepochs,\n",
    "            penal_coeff=penal_coeff, clip=clip, class_weight=class_weight, loss_name=loss_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions and sampling using our model\n",
    "\n",
    "Now that the model has been trained, we can randomly input a sentence into the model and predict its emotional value tag. The range of emotional markers (or the labels) is zero, one or two -- negative, neutral, positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "\n",
      "[[[0.08115912 0.13046429 0.23734735 0.55102926]\n",
      "  [0.02969767 0.06668776 0.1767809  0.7268337 ]]]\n",
      "<NDArray 1x2x4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "input_ar = nd.array(vocab[['this','phone','is','amazing']], ctx=mx.cpu()).reshape((1, -1))\n",
    "pred, att = model(input_ar)\n",
    "\n",
    "label = np.argmax(nd.softmax(pred, axis=1).asnumpy(), axis=1)\n",
    "print(label)\n",
    "print(att)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to intuitively understand the role of the attention mechanism, we visualize the output of the model's attention on the predicted samples using the `matplotlib` and `seaborn` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x72 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing the attention layer\n",
    "np.squeeze(att.asnumpy(), 0).shape\n",
    "plt.figure(figsize=(8,1))\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "sns.heatmap(np.squeeze(att.asnumpy(), 0), cmap=cmap, annot=True,\n",
    "            xticklabels=['this','phone','is','amazing'], yticklabels=['att0', 'att1'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
