{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Bag of Tweets\n",
    "\n",
    "####  Eryk Wdowiak and Eric Adsetts\n",
    "\n",
    "Module 4 project -- sentiment in Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from  nltk import FreqDist\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>direction</th>\n",
       "      <th>emotion</th>\n",
       "      <th>company</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>negative</td>\n",
       "      <td>Apple</td>\n",
       "      <td>device</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>positive</td>\n",
       "      <td>Apple</td>\n",
       "      <td>software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>positive</td>\n",
       "      <td>Apple</td>\n",
       "      <td>device</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>negative</td>\n",
       "      <td>Apple</td>\n",
       "      <td>software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>positive</td>\n",
       "      <td>Google</td>\n",
       "      <td>company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@teachntech00 New iPad Apps For #SpeechTherapy...</td>\n",
       "      <td>nan</td>\n",
       "      <td>neutral</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#SXSW is just starting, #CTIA is around the co...</td>\n",
       "      <td>Android</td>\n",
       "      <td>positive</td>\n",
       "      <td>Google</td>\n",
       "      <td>device</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Beautifully smart and simple idea RT @madebyma...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>positive</td>\n",
       "      <td>Apple</td>\n",
       "      <td>software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Counting down the days to #sxsw plus strong Ca...</td>\n",
       "      <td>Apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>Apple</td>\n",
       "      <td>company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Excited to meet the @samsungmobileus at #sxsw ...</td>\n",
       "      <td>Android</td>\n",
       "      <td>positive</td>\n",
       "      <td>Google</td>\n",
       "      <td>device</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweet           direction  \\\n",
       "0   .@wesley83 I have a 3G iPhone. After 3 hrs twe...              iPhone   \n",
       "1   @jessedee Know about @fludapp ? Awesome iPad/i...  iPad or iPhone App   \n",
       "2   @swonderlin Can not wait for #iPad 2 also. The...                iPad   \n",
       "3   @sxsw I hope this year's festival isn't as cra...  iPad or iPhone App   \n",
       "4   @sxtxstate great stuff on Fri #SXSW: Marissa M...              Google   \n",
       "5   @teachntech00 New iPad Apps For #SpeechTherapy...                 nan   \n",
       "7   #SXSW is just starting, #CTIA is around the co...             Android   \n",
       "8   Beautifully smart and simple idea RT @madebyma...  iPad or iPhone App   \n",
       "9   Counting down the days to #sxsw plus strong Ca...               Apple   \n",
       "10  Excited to meet the @samsungmobileus at #sxsw ...             Android   \n",
       "\n",
       "     emotion  company   product  \n",
       "0   negative    Apple    device  \n",
       "1   positive    Apple  software  \n",
       "2   positive    Apple    device  \n",
       "3   negative    Apple  software  \n",
       "4   positive   Google   company  \n",
       "5    neutral  unknown   unknown  \n",
       "7   positive   Google    device  \n",
       "8   positive    Apple  software  \n",
       "9   positive    Apple   company  \n",
       "10  positive   Google    device  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##  load data\n",
    "data = pd.read_csv('judge-1377884607_tweet_product_company_v2-clean.csv')\n",
    "data.columns = ['tweet','direction','emotion']\n",
    "# data.shape  # (9093, 3)\n",
    "\n",
    "##  remove rows without tweet\n",
    "data = data.dropna(subset=['tweet','emotion'],axis='index')\n",
    "# data.shape  # (9092, 3)\n",
    "\n",
    "##  clean emotions\n",
    "emo_dict = {'Negative emotion':'negative', \n",
    "            'Positive emotion':'positive',\n",
    "            'No emotion toward brand or product':'neutral', \n",
    "            \"I can't tell\":'neutral'}\n",
    "data['emotion'] = data['emotion'].replace(emo_dict)\n",
    "del emo_dict\n",
    "\n",
    "##  define company and product\n",
    "##  first convert NaN to a string\n",
    "data['direction'] = data['direction'].map('{}'.format)\n",
    "\n",
    "##  define company\n",
    "comp_dict = {'iPhone':'Apple', \n",
    "             'iPad or iPhone App':'Apple', \n",
    "             'iPad':'Apple', \n",
    "             'Google':'Google', \n",
    "             'nan':'unknown', \n",
    "             'Android':'Google',\n",
    "             'Apple':'Apple',\n",
    "             'Android App':'Google', \n",
    "             'Other Google product or service':'Google',\n",
    "             'Other Apple product or service':'Apple'}\n",
    "data['company'] = data['direction'].replace(comp_dict)\n",
    "del comp_dict\n",
    "\n",
    "##  define product\n",
    "prod_dict = {'iPhone':'device', \n",
    "             'iPad or iPhone App':'software', \n",
    "             'iPad':'device', \n",
    "             'Google':'company', \n",
    "             'nan':'unknown', \n",
    "             'Android':'device',\n",
    "             'Apple':'company',\n",
    "             'Android App':'software', \n",
    "             'Other Google product or service':'other',\n",
    "             'Other Apple product or service':'other'}\n",
    "data['product'] = data['direction'].replace(prod_dict)\n",
    "del prod_dict\n",
    "\n",
    "##  let's take a look\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  prepare stop word list\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list += list(string.punctuation)\n",
    "stopwords_list += ['0','1','2','3','4','5','6','7','8','9']\n",
    "\n",
    "##  process tweets\n",
    "def process_tweets(tweet):\n",
    "    tokens = word_tokenize(tweet)\n",
    "    wnl = WordNetLemmatizer()\n",
    "    no_stop_lemmas = [wnl.lemmatize(token.lower()) for token in tokens if token.lower() not in stopwords_list]\n",
    "    ot_string = ' '.join(no_stop_lemmas)\n",
    "    return ot_string\n",
    "\n",
    "##  process tweets\n",
    "data['tweet'] = list(map(process_tweets, list(data['tweet'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[['tweet','company','product']], \n",
    "                                                    data['emotion'], \n",
    "                                                    test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  create TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer()\n",
    "tf_idf_data_train = vectorizer.fit_transform(X_train['tweet'])\n",
    "tf_idf_data_test = vectorizer.transform(X_test['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes\n",
      "Training Accuracy: 0.8725\n",
      "Testing Accuracy:  0.6619\n",
      "Training f1: 0.8691\n",
      "Testing f1:  0.6435\n"
     ]
    }
   ],
   "source": [
    "##  naive bayes classifier\n",
    "nb_classifier = MultinomialNB(alpha = .1)\n",
    "\n",
    "nb_classifier.fit(tf_idf_data_train, y_train)\n",
    "nb_train_preds = nb_classifier.predict(tf_idf_data_train)\n",
    "nb_test_preds = nb_classifier.predict(tf_idf_data_test)\n",
    "\n",
    "nb_train_score = accuracy_score(y_train, nb_train_preds)\n",
    "nb_test_score = accuracy_score(y_test, nb_test_preds)\n",
    "nb_train_f1 = f1_score(y_train, nb_train_preds, average='weighted')\n",
    "nb_test_f1 = f1_score(y_test, nb_test_preds, average = 'weighted')\n",
    "\n",
    "print(\"Multinomial Naive Bayes\")\n",
    "print(\"Training Accuracy: {:.4}\".format(nb_train_score))\n",
    "print(\"Testing Accuracy:  {:.4}\".format(nb_test_score))\n",
    "print(\"Training f1: {:.4}\".format(nb_train_f1))\n",
    "print(\"Testing f1:  {:.4}\".format(nb_test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6686359704476807\n",
      "{'alpha': 0.25}\n",
      "0.6274239305055128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "#hyper parameter tuning\n",
    "nb_grid = GridSearchCV(nb_classifier, {'alpha': [.01,.1,.25,.5,1]}, n_jobs = -1, verbose = 1)\n",
    "nb_grid.fit(tf_idf_data_train, y_train)\n",
    "print(nb_grid.best_score_)\n",
    "print(nb_grid.best_params_)\n",
    "nb_grid_preds = nb_grid.predict(tf_idf_data_test)\n",
    "nb_grid_f1 = f1_score(y_test, nb_grid_preds, average = 'weighted')\n",
    "print(nb_grid_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.25)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_optimal = MultinomialNB(alpha = .25)\n",
    "nb_optimal.fit(tf_idf_data_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forests\n",
      "Training Accuracy: 0.9945\n",
      "Testing Accuracy:  0.6707\n",
      "Training f1: 0.9945\n",
      "Testing f1:  0.6347\n"
     ]
    }
   ],
   "source": [
    "##  random forests classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=1000, class_weight = 'balanced', max_depth = None, n_jobs = -1)\n",
    "\n",
    "rf_classifier.fit(tf_idf_data_train, y_train)\n",
    "rf_train_preds = rf_classifier.predict(tf_idf_data_train)\n",
    "rf_test_preds = rf_classifier.predict(tf_idf_data_test)\n",
    "\n",
    "rf_train_score = accuracy_score(y_train, rf_train_preds)\n",
    "rf_test_score = accuracy_score(y_test, rf_test_preds)\n",
    "rf_train_f1 = f1_score(y_train, rf_train_preds, average='weighted')\n",
    "rf_test_f1 = f1_score(y_test, rf_test_preds, average = 'weighted')\n",
    "    \n",
    "print('Random Forests')\n",
    "print(\"Training Accuracy: {:.4}\".format(rf_train_score))\n",
    "print(\"Testing Accuracy:  {:.4}\".format(rf_test_score))\n",
    "print(\"Training f1: {:.4}\".format(rf_train_f1))\n",
    "print(\"Testing f1:  {:.4}\".format(rf_test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  7.4min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  9.1min\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed: 13.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6806003110272881\n",
      "{'max_depth': None, 'min_samples_split': 20, 'n_estimators': 500}\n",
      "0.6562396532817701\n"
     ]
    }
   ],
   "source": [
    "#hyper parameter tuning\n",
    "rf_grid = GridSearchCV(rf_classifier, {'n_estimators' : [100,250,500, 1000],\n",
    "                                       'max_depth': [None, 1,2,3,4,5,10,15,20],\n",
    "                                       'min_samples_split': [2,6,10,20]},\n",
    "                       n_jobs = -1, verbose = 1)\n",
    "rf_grid.fit(tf_idf_data_train, y_train)\n",
    "print(rf_grid.best_score_)\n",
    "print(rf_grid.best_params_)\n",
    "rf_grid_preds = rf_grid.predict(tf_idf_data_test)\n",
    "rf_grid_f1 = f1_score(y_test, rf_grid_preds, average = 'weighted')\n",
    "print(rf_grid_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_optimal = RandomForestClassifier(max_depth = None, min_samples_split = 20, n_estimators = 500, class_weight = 'balanced', n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Training Accuracy: 0.8203\n",
      "Testing Accuracy:  0.6366\n",
      "Training f1: 0.8237\n",
      "Testing f1:  0.6436\n"
     ]
    }
   ],
   "source": [
    "#Log Reg\n",
    "logreg = LogisticRegression(class_weight = 'balanced', n_jobs = -1)\n",
    "\n",
    "logreg.fit(tf_idf_data_train, y_train)\n",
    "lr_train_preds = logreg.predict(tf_idf_data_train)\n",
    "lr_test_preds = logreg.predict(tf_idf_data_test)\n",
    "\n",
    "lr_train_score = accuracy_score(y_train, lr_train_preds)\n",
    "lr_test_score = accuracy_score(y_test, lr_test_preds)\n",
    "lr_train_f1 = f1_score(y_train, lr_train_preds, average='weighted')\n",
    "lr_test_f1 = f1_score(y_test, lr_test_preds, average = 'weighted')\n",
    "    \n",
    "print('Logistic Regression')\n",
    "print(\"Training Accuracy: {:.4}\".format(lr_train_score))\n",
    "print(\"Testing Accuracy:  {:.4}\".format(lr_test_score))\n",
    "print(\"Training f1: {:.4}\".format(lr_train_f1))\n",
    "print(\"Testing f1:  {:.4}\".format(lr_test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 13 candidates, totalling 65 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  65 | elapsed:    3.7s remaining:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done  65 out of  65 | elapsed:    4.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6727636523490123\n",
      "{'C': 20}\n",
      "0.64768855624222\n"
     ]
    }
   ],
   "source": [
    "#hyper parameter tuning\n",
    "lr_grid = GridSearchCV(logreg, {'C': [-1,-.5,-.1,.1,.5,1,1.5,2,2.5,10,20,25,50]},\n",
    "                       n_jobs = -1, verbose = 1)\n",
    "lr_grid.fit(tf_idf_data_train, y_train)\n",
    "print(lr_grid.best_score_)\n",
    "print(lr_grid.best_params_)\n",
    "lr_grid_preds = lr_grid.predict(tf_idf_data_test)\n",
    "lr_grid_f1 = f1_score(y_test, lr_grid_preds, average = 'weighted')\n",
    "print(lr_grid_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_optimal = LogisticRegression(class_weight = 'balanced', n_jobs = -1, C = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Training Accuracy: 0.9945\n",
      "Testing Accuracy:  0.5937\n",
      "Training f1: 0.9945\n",
      "Testing f1:  0.5948\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree\n",
    "tree = DecisionTreeClassifier(class_weight = 'balanced')\n",
    "\n",
    "tree.fit(tf_idf_data_train, y_train)\n",
    "dt_train_preds = tree.predict(tf_idf_data_train)\n",
    "dt_test_preds = tree.predict(tf_idf_data_test)\n",
    "\n",
    "dt_train_score = accuracy_score(y_train, dt_train_preds)\n",
    "dt_test_score = accuracy_score(y_test, dt_test_preds)\n",
    "dt_train_f1 = f1_score(y_train, dt_train_preds, average='weighted')\n",
    "dt_test_f1 = f1_score(y_test, dt_test_preds, average = 'weighted')\n",
    "    \n",
    "print('Decision Tree')\n",
    "print(\"Training Accuracy: {:.4}\".format(dt_train_score))\n",
    "print(\"Testing Accuracy:  {:.4}\".format(dt_test_score))\n",
    "print(\"Training f1: {:.4}\".format(dt_train_f1))\n",
    "print(\"Testing f1:  {:.4}\".format(dt_test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 175 candidates, totalling 875 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   13.6s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   27.4s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:   36.5s\n",
      "[Parallel(n_jobs=-1)]: Done 875 out of 875 | elapsed:   37.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5799513133576294\n",
      "{'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.5885876646880155\n"
     ]
    }
   ],
   "source": [
    "#hyper parameter tuning\n",
    "dt_grid = GridSearchCV(tree, {'max_depth': [None, 50,25,10,5], 'min_samples_split': [2,4,8,10,12,14,16],\n",
    "                             'min_samples_leaf': [1,5,10,15,20]},\n",
    "                       n_jobs = -1, verbose = 1)\n",
    "dt_grid.fit(tf_idf_data_train, y_train)\n",
    "print(dt_grid.best_score_)\n",
    "print(dt_grid.best_params_)\n",
    "dt_grid_preds = dt_grid.predict(tf_idf_data_test)\n",
    "dt_grid_f1 = f1_score(y_test, dt_grid_preds, average = 'weighted')\n",
    "print(dt_grid_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_optimal = DecisionTreeClassifier(class_weight = 'balanced', max_depth = None, min_samples_leaf = 1, min_samples_split = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Nearest Neighbors\n",
      "Training Accuracy: 0.7489\n",
      "Testing Accuracy:  0.6328\n",
      "Training f1: 0.7324\n",
      "Testing f1:  0.6044\n"
     ]
    }
   ],
   "source": [
    "#KNN\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "knn.fit(tf_idf_data_train, y_train)\n",
    "knn_train_preds = knn.predict(tf_idf_data_train)\n",
    "knn_test_preds = knn.predict(tf_idf_data_test)\n",
    "\n",
    "knn_train_score = accuracy_score(y_train, knn_train_preds)\n",
    "knn_test_score = accuracy_score(y_test, knn_test_preds)\n",
    "knn_train_f1 = f1_score(y_train, knn_train_preds, average='weighted')\n",
    "knn_test_f1 = f1_score(y_test, knn_test_preds, average = 'weighted')\n",
    "    \n",
    "print('K Nearest Neighbors')\n",
    "print(\"Training Accuracy: {:.4}\".format(knn_train_score))\n",
    "print(\"Testing Accuracy:  {:.4}\".format(knn_test_score))\n",
    "print(\"Training f1: {:.4}\".format(knn_train_f1))\n",
    "print(\"Testing f1:  {:.4}\".format(knn_test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:   13.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6462250835472236\n",
      "{'n_neighbors': 11, 'p': 2}\n",
      "0.6017880255599793\n"
     ]
    }
   ],
   "source": [
    "#hyper parameter tuning\n",
    "knn_grid = GridSearchCV(knn, {'n_neighbors': [1,3,5,7,9,11,13,15], 'p': [1,2,3]},\n",
    "                       n_jobs = -1, verbose = 1)\n",
    "knn_grid.fit(tf_idf_data_train, y_train)\n",
    "print(knn_grid.best_score_)\n",
    "print(knn_grid.best_params_)\n",
    "knn_grid_preds = knn_grid.predict(tf_idf_data_test)\n",
    "knn_grid_f1 = f1_score(y_test, knn_grid_preds, average = 'weighted')\n",
    "print(knn_grid_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_optimal = KNeighborsClassifier(n_neighbors = 11, p = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machines\n",
      "Training Accuracy: 0.912\n",
      "Testing Accuracy:  0.6658\n",
      "Training f1: 0.9126\n",
      "Testing f1:  0.6619\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "svm = SVC(class_weight = 'balanced')\n",
    "\n",
    "svm.fit(tf_idf_data_train, y_train)\n",
    "svm_train_preds = svm.predict(tf_idf_data_train)\n",
    "svm_test_preds = svm.predict(tf_idf_data_test)\n",
    "\n",
    "svm_train_score = accuracy_score(y_train, svm_train_preds)\n",
    "svm_test_score = accuracy_score(y_test, svm_test_preds)\n",
    "svm_train_f1 = f1_score(y_train, svm_train_preds, average='weighted')\n",
    "svm_test_f1 = f1_score(y_test, svm_test_preds, average = 'weighted')\n",
    "    \n",
    "print('Support Vector Machines')\n",
    "print(\"Training Accuracy: {:.4}\".format(svm_train_score))\n",
    "print(\"Testing Accuracy:  {:.4}\".format(svm_test_score))\n",
    "print(\"Training f1: {:.4}\".format(svm_train_f1))\n",
    "print(\"Testing f1:  {:.4}\".format(svm_test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 28 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   42.5s\n",
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:  2.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6891237822430833\n",
      "{'C': 25, 'kernel': 'rbf'}\n",
      "0.6597462263976194\n"
     ]
    }
   ],
   "source": [
    "#hyper parameter tuning\n",
    "svm_grid = GridSearchCV(svm, {'C': [.5,1,5,10,15,25,50],\n",
    "                             'kernel': ['linear', 'poly', 'rbf', 'sigmoid']},\n",
    "                       n_jobs = -1, verbose = 1)\n",
    "svm_grid.fit(tf_idf_data_train, y_train)\n",
    "print(svm_grid.best_score_)\n",
    "print(svm_grid.best_params_)\n",
    "svm_grid_preds = svm_grid.predict(tf_idf_data_test)\n",
    "svm_grid_f1 = f1_score(y_test, svm_grid_preds, average = 'weighted')\n",
    "print(svm_grid_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_optimal = SVC(class_weight = 'balanced', C = 25, kernel = 'rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:25:14] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { num_classes } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "Support Vector Machines\n",
      "Training Accuracy: 0.8471\n",
      "Testing Accuracy:  0.6636\n",
      "Training f1: 0.8374\n",
      "Testing f1:  0.6323\n"
     ]
    }
   ],
   "source": [
    "xgb_clf = xgb.XGBClassifier(objective = 'multi:softmax', num_classes = 3, n_jobs = -1, \n",
    "                            verbosity = 1)\n",
    "\n",
    "xgb_clf.fit(tf_idf_data_train, y_train)\n",
    "xgb_train_preds = xgb_clf.predict(tf_idf_data_train)\n",
    "xgb_test_preds = xgb_clf.predict(tf_idf_data_test)\n",
    "\n",
    "xgb_train_score = accuracy_score(y_train, xgb_train_preds)\n",
    "xgb_test_score = accuracy_score(y_test, xgb_test_preds)\n",
    "xgb_train_f1 = f1_score(y_train, xgb_train_preds, average='weighted')\n",
    "xgb_test_f1 = f1_score(y_test, xgb_test_preds, average = 'weighted')\n",
    "    \n",
    "print('Support Vector Machines')\n",
    "print(\"Training Accuracy: {:.4}\".format(xgb_train_score))\n",
    "print(\"Testing Accuracy:  {:.4}\".format(xgb_test_score))\n",
    "print(\"Training f1: {:.4}\".format(xgb_train_f1))\n",
    "print(\"Testing f1:  {:.4}\".format(xgb_test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper parameter tuning\n",
    "xgb_grid = GridSearchCV(xgb_clf, {},\n",
    "                       n_jobs = -1, verbose = 1)\n",
    "xgb_grid.fit(tf_idf_data_train, y_train)\n",
    "print(xgb_grid.best_score_)\n",
    "print(xgb_grid.best_params_)\n",
    "xgb_grid_preds = xgb_grid.predict(tf_idf_data_test)\n",
    "xgb_grid_f1 = f1_score(y_test, xgb_grid_preds, average = 'weighted')\n",
    "print(xgb_grid_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier\n",
      "Training Accuracy: 0.975\n",
      "Testing Accuracy:  0.6905\n",
      "Training f1: 0.9749\n",
      "Testing f1:  0.6705\n"
     ]
    }
   ],
   "source": [
    "voting_clf = VotingClassifier(estimators = [('nb', nb_optimal), ('rf', rf_optimal), ('lr', lr_optimal),('tree', dt_optimal), \n",
    "                                            ('knn', knn_optimal), ('svm', svm_optimal), ('xgb', xgb_clf)], n_jobs = -1)\n",
    "\n",
    "voting_clf.fit(tf_idf_data_train, y_train)\n",
    "voting_train_preds = voting_clf.predict(tf_idf_data_train)\n",
    "voting_test_preds = voting_clf.predict(tf_idf_data_test)\n",
    "\n",
    "voting_train_score = accuracy_score(y_train, voting_train_preds)\n",
    "voting_test_score = accuracy_score(y_test, voting_test_preds)\n",
    "voting_train_f1 = f1_score(y_train, voting_train_preds, average='weighted')\n",
    "voting_test_f1 = f1_score(y_test, voting_test_preds, average = 'weighted')\n",
    "    \n",
    "print('Voting Classifier')\n",
    "print(\"Training Accuracy: {:.4}\".format(voting_train_score))\n",
    "print(\"Testing Accuracy:  {:.4}\".format(voting_test_score))\n",
    "print(\"Training f1: {:.4}\".format(voting_train_f1))\n",
    "print(\"Testing f1:  {:.4}\".format(voting_test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
