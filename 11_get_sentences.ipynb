{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  tweets to text\n",
    "\n",
    "#### Eryk Wdowiak and Eric Adsetts\n",
    "\n",
    "save tweets to text file, which we'll use for subword splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "# from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  load data\n",
    "data = pd.read_csv('dataset/judge-1377884607_tweet_product_company_v2-clean.csv')\n",
    "data.columns = ['tweet','direction','emotion']\n",
    "# data.shape  # (9093, 3)\n",
    "\n",
    "##  remove rows without tweet\n",
    "data = data.dropna(subset=['tweet','emotion'],axis='index')\n",
    "# data.shape  # (9092, 3)\n",
    "\n",
    "##  clean emotions\n",
    "emo_dict = {'Negative emotion':'negative', \n",
    "            'Positive emotion':'positive',\n",
    "            'No emotion toward brand or product':'neutral', \n",
    "            \"I can't tell\":'neutral'}\n",
    "data['emotion'] = data['emotion'].replace(emo_dict)\n",
    "del emo_dict\n",
    "\n",
    "emo_int_dict = {'negative':0, 'neutral':1, 'positive':2 } \n",
    "data['emot_int'] = data['emotion'].replace(emo_int_dict)\n",
    "del emo_int_dict\n",
    "\n",
    "##  define company and product\n",
    "##  first convert NaN to a string\n",
    "data['direction'] = data['direction'].map('{}'.format)\n",
    "\n",
    "##  define company\n",
    "comp_dict = {'iPhone':'Apple', \n",
    "             'iPad or iPhone App':'Apple', \n",
    "             'iPad':'Apple', \n",
    "             'Google':'Google', \n",
    "             'nan':'unknown', \n",
    "             'Android':'Google',\n",
    "             'Apple':'Apple',\n",
    "             'Android App':'Google', \n",
    "             'Other Google product or service':'Google',\n",
    "             'Other Apple product or service':'Apple'}\n",
    "data['company'] = data['direction'].replace(comp_dict)\n",
    "del comp_dict\n",
    "\n",
    "##  define product\n",
    "prod_dict = {'iPhone':'device', \n",
    "             'iPad or iPhone App':'software', \n",
    "             'iPad':'device', \n",
    "             'Google':'company', \n",
    "             'nan':'unknown', \n",
    "             'Android':'device',\n",
    "             'Apple':'company',\n",
    "             'Android App':'software', \n",
    "             'Other Google product or service':'other',\n",
    "             'Other Apple product or service':'other'}\n",
    "data['product'] = data['direction'].replace(prod_dict)\n",
    "del prod_dict\n",
    "\n",
    "##  let's take a look\n",
    "# data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  prepare stop word list\n",
    "stopwords_list = []\n",
    "# stopwords_list += stopwords.words('english')\n",
    "stopwords_list += list(string.punctuation)\n",
    "# stopwords_list += ['0','1','2','3','4','5','6','7','8','9']\n",
    "\n",
    "##  how to process tweets\n",
    "def process_tweets(tweet):\n",
    "    lower_text = tweet.replace('\\n',' ').lower()\n",
    "    tokens = word_tokenize(lower_text)\n",
    "    tokens = [token for token in tokens if token not in stopwords_list]\n",
    "    token_str = ' '.join(tokens)\n",
    "    token_str = token_str.replace(' \\'','\\'').replace('/',' ')\n",
    "    return token_str\n",
    "\n",
    "##  process tweets\n",
    "data['tweet'] = list(map(process_tweets, list(data['tweet'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data[['tweet','direction','company','product']], data['emotion'], \n",
    "    test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  rejoin them\n",
    "data_train = X_train.join(y_train)\n",
    "data_test  = X_test.join(y_test)\n",
    "\n",
    "##  save them\n",
    "data_train.to_csv('dataset/judge-1377884607_zz_data-train.csv',index=None)\n",
    "data_test.to_csv('dataset/judge-1377884607_zz_data-test.csv',index=None)\n",
    "\n",
    "##  and save tweets separately\n",
    "data_train['tweet'].to_csv('dataset/judge-1377884607_zz_tweets-train_v0.txt',\n",
    "                           header=None, index=None, sep=' ')\n",
    "data_test['tweet'].to_csv('dataset/judge-1377884607_zz_tweets-test_v0.txt',\n",
    "                          header=None, index=None, sep=' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
